---
layout: single
title: "Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment (3)"
author: DaYun Choi
category: PaperReview
tags: audio visual self-supervised AVC AVSA classification detection
excerpt: # "First post"
date: "2022-06-24 21:40"
last_modified_at: "2022-06-27 02:40"
published: true
comments: true
toc: true
toc_sticky: true
use_kakao-sdk: true
use_math: true
---

## Experimental Results
제안한 AVC와 AVSA를 in-domain에서 평가하고,  
visual 만을 다룬 downstream task에 적용한 뒤, 
> 학습은 random으로 선택한 image crop 기반, audio 특징 표현은 log-mel spectrum으로 구성 $\rightarrow$ baseline system

audio downstream task에 집중하고자 한다.

### Dataset and data format verification
학습은 spatial audio와 함께 360도 video를 포함하는 YouTube-360 데이터셋을 기반으로 한다.  
총 5506개의 데이터가 있으며, 이 중 4506개는 학습에, 1000개는 평가에 사용된다.  
silent region을 피하기 위해 automatic curation*이 먼저 수행되고 각 영상을 10초짜리 clip으로 잘라 특정 threshold를 넘는 clip만 사용한다.  
> curation은 목적에 따라 가치 있게 구성하는 것이라 보면 된다.

결과적으로 총 88733개의 clip을 얻을 수 있고 각각은 unlabeled 되어 있다.  
자세한 과정은 Morgado et al. 논문에 나와 있다고 한다.

---

제안한 방식은 ambisonic의 spatial encoding에 의존하기 때문에, 그리고 그 encoding이 추가적인 metadata가 아닌 그것들 자체의 audio signal로 통합된 것이기 때문에 error 없이 beamforming과 rotation을 잘 하려면 channel ordering과 channel normalization convention을 아는 것이 매우 중요하다.  
전자의 경우 FOA에 대해 WYZX의 순서로 정렬하는 _ACN_ channel ordering, 후자의 경우 _ambiX_ 형태로 알려진 _SN3D_ channel normalization의 방식을 이용했다.  
그러나 ambisonic은 아직 audio file container 형태로 정의된 것이 아니기 때문에 다른 surround format과 channel swapping을 헷갈리는 경우가 종종 일어난다.  
이런 channel oredering을 올바르게 하기 위해서는 channel order를 알고 있는 영상을 YouTube에 업로드 한 다음에 다운 받아서 다시 [_pyAV_](https://github.com/PyAV-Org/PyAV){:target="_blank"}를 이용하여 다시 업로드 한다.  
이 방식을 이용하면 모든 영상에 대해 올바르게 WYZX mapping이 가능하다.

그러나 여전히 많은 양의 파일이 ambisonic forat specification을 만족하지 않을 수도 있다.  
이런 일이 일어나는 이유는
- user가 ambisonic file이 아닌 stereo file로 업로드
- 잘못된 channel order로 ambisonic file을 업로드
- recording device나 encoding software가 잘못된 encoding 제공

와 같은 경우가 있다.  
YouTube에 올라오는 영상이 워낙 다양해서 이런 invalid 하는 경우가 많이 일어나는 것이다.  
따라서 이상적인 ambisonic recording과 얼마나 가까운지를 측정하는 test를 실행한다.  
$E_w=\sum_{n}w(n)^2$을 omnidirectional channel의 energy, $E_{xyz}=\sum_{n}(x(n)^2+y(n)^2+z(n)^2)$을 세 dipole channel의 energy라고 하면 thresohold인 $\tau<1$에 대해 다음 식으로 test한다.  
\\[ \left | \frac{E_{xyz}}{E_w}-1 \right |\leq\tau \\]  
이 식의 의미는 sound scene에서 uncorrelated sources, encoding의 orthogonality로 인한 incoherent diffuse ambience와 reverberation의 조합과 같은 여러 상황에 대해 $E_w$가 $E_{xyz}$와 최대한 같도록 하는 것이다.  
ambisonic을 녹화하는 장치는 high-frequency에서 spatial aliasing을 겪기 때문에 4kHz 기준의 low-pass filter를 통과시키고 $\tau=0.1$로 설정했더니 처음 88733개의 clip 중 28%만이 남았다.

---

AVC와 AVSA에 대한 평가를 세 가지 case로 나누어 볼 것인데 다음과 같다.
- complete training subset
- 앞서 언급한 test를 통과한 28% files
- test를 통과하지 못한 것 중 나머지 training subset에서 random 으로 선택한 28% files

선택되지 않은 file들은 모두 test set으로 활용되며, 따라서 test set은 완전히 ambisonic이냐, 아니냐에 따라 나뉜다.  
baseline system setup을 이용하여 AVC와 AVSA를 훈련시킨 결과는 다음과 같다.  
AVC는 orientation에 따라 변하지 않고 correspondence만 배우기 때문에 충분한 inconsistency와 variety가 있어 잘못 회전된 다양한 데이터로부터 이득을 얻는 것처럼 보인다.  
엄격하게 ambisonic만 뽑은 데이터는 공간적으로 homogeneous 하기 때문에 상대적으로 좋지 않은 performance를 보여준다.  
한편 AVSA는 AVC에 비해 훨씬 더 낮은 accuracy를 보인다.  
한 가지 요소를 추가적으로 고려해야 되기 때문에 아무래도 이러한 결과가 나온 것으로 추측할 수 있다.  
두 경우 모두 이상적인 ambisonic encoding으로부터 편차가 있어서 전체 데이터를 모두 사용하는 것이 더 좋은 결과를 얻었기에 다음 실험들은 모두 전체 데이터를 이용한다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175816100-2752a5b1-ebae-4d42-af25-13a838cd07ef.png">
</p>

### Audio-visual correspondence and spatial alignment
baseline system부터 시작해서 앞서 언급한 모든 요소를 추가해 가며 얻은 결과는 다음 표와 같다.  
가장 주목할만한 부분은 바로 두번째 칸의 FOA를 사용함으로써 상당한 성능 향상을 보였다.  
여기에서 FOA-IV를 사용하니 spatial alignment의 이점을 가지게 되어 AVSA가 더 좋은 성능을 보일 수 있게 되었다.  
stereo의 경우 baseline과 비교했을 때 AVC는 성능이 크게 감소했으나 AVSA는 그렇게 크게 감소하지는 않았다.  
beamforming은 그렇게 큰 효과를 가지고 오지 않았다는 것을 알 수 있다.

이러한 결과를 기반으로 봤을 때 audio signal을 나타내기 위해 spatial information을 사용하는 것은 in-domain task에서 더 좋은 결과를 가져온다는 것을 알 수 있다.  
random crop 대신 YOLO를 사용한 것이 좋은 결과를 보인 반면, stereo에는 그렇게 큰 효과가 없었다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175816087-d4ffafc0-46be-4318-9194-b09b2af4d94a.png">
</p>

### Human action recognition
이번 task에서는 101개의 class로 이루어진 UCF dataset과 51개의 class로 이루어진 HMDB dataset을 사용해보자.  
이는 video embedding만을 사용하고 audio content는 포함되지 않는다.  
결과를 검증하는 방법은 학습된 feature들을 embedding으로 사용하여 각 dataset의 class 개수만큼의 neuron으로 이루어진 simple linear layer를 추가한다.  
이 layer의 parameter는 학습되겠지만 pretrained network의 parameter는 고정시킨다.  
또 다른 방법으로는 전체 network의 paramter를 update 하여 domain의 mismatch를 보완한다. ($\rightarrow$ fine tuning)  
fine tuning을 한 것과 하지 않은 것에 대한 결과는 다음 표와 같다.  
먼저 fine tuning을 하지 않은 경우에 대해서 보면 UCF dataset에 대해 여러 요소들을 추가하는 것이 baseline보다 거의 좋은 성능을 보였고 특히 beamforming과 YOLO를 함께 사용하는 것이 제일 좋은 결과를 보였다. HMDB dataset에 대해서는 stereo와 YOLO를 함께 사용하는 것이 제일 좋은 결과를 보였다.  
한편 fine tuning을 한 경우에 대해서 보면 여러 요소를 추가한 것이 상대적으로 그렇게 큰 효과를 가져오지는 않았다. UCF dataset에 대해서는 FOA과 FOA-IV를 함께 사용하는 것이, HMDB dataset에 대해서는 beamforming 또는 stereo를 YOLO와 함께 사용하는 것이 제일 좋은 결과를 보였다.

이 결과는 마찬가지로 spatial information을 사용하는 것이 좋은 결과를 가져온다는 것을 보여주며, in-domain task 뿐만 아니라 video domain에서도 좋은 성능을 보인다는 것이다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175816090-24008ed4-f09d-4d05-8cc3-fea797a53b42.png">
</p>

### Acoustic scene classification with ambisonic audio
앞서서는 video domain에서 보았으니 이제 audio domain에서 확인해보자.  
8개 class이면서 총 640분으로 구성된 Eigenscape dataset을 사용하고자 한다.  
바람으로 인한 noise를 막고 ambisonic으로 변환하기 위해 mh Acoustics Eigenmike를 사용하여 녹화된 것이다.  
각 녹화된 음성은 30초 segment로 분할되어 cross validation을 할 수 있도록 한다.  
이에 대한 결과는 다음 표와 같으며 fine tuning은 진행하지 않았다.  
baseline에 대한 결과는 AVSA가 더 좋은 결과를 보인 것으로 보아 spatial correspondence가 고려될 때 학습된 표현의 superiority가 있다는 것을 알 수 있다.  
한편 FOA-IV에 대한 결과를 보니 AVC에는 영향이 별로 없으나 AVSA에는 상당한 성능 감소를 보였다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175816091-14bccb50-beb8-4e81-b84e-706248a1ab09.png">
</p>

### Acoustic scene classification with binaural audio
마지막으로 ambisonic이 아닌 binaural audio에 대한 실험을 진행하였다.  
system을 훈련하기 위해 사용한 stereo format과 test를 위해 사용한 audio data의 binaural format 사이의 mismatch가 있다.  
특히 주된 차이는 binaural format의 head-related transfer functions(HRTFs)*와 stereo format의 유효한 ambisonic encoding에 대해 거의 frequency-independent 한 level difference를 포함하는 것이다.  
> low-mid frequency에서는 강한 inter-aural time differences, mid-high frequency에서는 frequency-varying level differences

실험은 10개의 class로 이루어졌으며 총 34시간 분량의 TAU Audio-Visual Urban Scenes 2021 dataset을 사용하였다.  
audio는 binaural in-ear microphone을 이용하여 녹화됐으며 video는 GoPro camera를 이용하여 녹화되었다.  
녹화하는 동안 녹화하는 사람은 움직이지 않았으며 각 clip마다 위치는 고정되었다.  
실험은 audio-only, video-only, early fusion of audio-video modalities의 세 가지 case로 진행되었으며 결과는 다음 표와 같다.  
fine tuning을 하지 않은 경우에는 baseline인 Open L3 system이 가장 좋은 성능을 보인 반면, fine tuning을 한 경우에는 YOLO의 방법을 적용하였더니 성능 향상을 보였고 ICF의 방법을 적용하였더니 오히려 성능 감소를 보였다.  

OpenL3로 생성된 embedding이 더 좋은 결과를 낳게 된 것은 본 논문이 제안한 것과 dataset, network architecture, optimization mechanism, loss function이 다르기 때문이다.  
그러나 AVSA는 그 자체로 좋은 성능을 보였기 때문에 spatial information을 사용하는 것이 더 좋다는 결론을 내릴 수 있다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175816093-a5f1d323-b75d-4c85-ad24-76dbc647d0d4.png">
</p>

## Conclusion and Future Work
다시 요약해보면 본 논문은 audio와 video information 사이의 spatial alignment를 기반으로 audio representation에 대해 학습하기 위해 self-supervised learning을 진행했다.  
audio와 video 사이의 강한 correspondence를 만들기 위해 YOLO를 사용하여 video frame에서 object detection을 하였고, ambisonic으로부터 spatial audio information을 얻기 위해 acoustic intensity vector를 사용하였다.  
위 여러 개의 실험 결과들을 통해 spatial information을 함께 사용함으로써 sound event classification, localization, tagging 등을 위해 다양한 direction information을 얻을 수 있다는 것을 알 수 있다.  
알다시피 YOLO는 일반적인 이미지에 훈련된 네트워크이므로 미래에는 equirectangular 이미지에도 잘 적용되도록 더 정교한 detection 방법을 연구하는 것이 더 좋을 것 같다.  
또한 FOA로부터 acoustic imaging을 하는 것에 대해 더욱 advanced 한 방법을 이용해 sound activity의 direction을 기반으로 한 interest area를 고르는 방법도 있을 것 같다.

---

이렇게 논문 한 편에 대해 정리해 보았다.  
category는 PaperReview라고 해놓았지만 사실 내용을 이해하기 위한 정리에 가깝다.  
중간중간엔 거의 번역 식으로만 써놓은 부분이 있는데 그 부분은 이해가 아직 잘 안돼서 그런 것 같다.  
나름 이해해 볼려고는 노력했지만 잘 된 부분도 있는 반면 잘 안 된 부분에 대해서는 배경지식이 아직 많이 부족해서라고 생각한다.  
앞으로 이러한 multimodal 분야에 대해 더 많은 논문과 자료를 찾아보면서 지식을 쌓아 나가야겠다.

_추후에 이 논문에 대한 3개의 포스트에 대해 좀 더 매끄러운 문장을 사용하도록 수정할 수 있습니다~_