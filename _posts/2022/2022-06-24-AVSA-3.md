---
layout: single
title: "Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment (3)"
author: DaYun Choi
category: PaperReview
tags: audio visual self-supervised AVC AVSA classification detection
excerpt: # "First post"
date: "2022-06-24 21:40"
last_modified_at: "2022-06-26 17:40"
published: true
comments: true
toc: true
toc_sticky: true
use_kakao-sdk: true
use_math: true
---

## Experimental Results
제안한 AVC와 AVSA를 in-domain에서 평가하고,  
visual 만을 다룬 downstream task에 적용한 뒤, 
> 학습은 random으로 선택한 image crop 기반, audio 특징 표현은 log-mel spectrum으로 구성 $\rightarrow$ baseline system

audio downstream task에 집중하고자 한다.

### Dataset and data format verification
학습은 spatial audio와 함께 360도 video를 포함하는 YouTube-360 데이터셋을 기반으로 한다.  
총 5506개의 데이터가 있으며, 이 중 4506개는 학습에, 1000개는 평가에 사용된다.  
silent region을 피하기 위해 automatic curation*이 먼저 수행되고 각 영상을 10초짜리 clip으로 잘라 특정 threshold를 넘는 clip만 사용한다.  
> curation은 목적에 따라 가치 있게 구성하는 것이라 보면 된다.

결과적으로 총 88733개의 clip을 얻을 수 있고 각각은 unlabeled 되어 있다.  
자세한 과정은 Morgado et al. 논문에 나와 있다고 한다.

---

제안한 방식은 ambisonic의 spatial encoding에 의존하기 때문에, 그리고 그 encoding이 추가적인 metadata가 아닌 그것들 자체의 audio signal로 통합된 것이기 때문에 error 없이 beamforming과 rotation을 잘 하려면 channel ordering과 channel normalization convention을 아는 것이 매우 중요하다.  
전자의 경우 FOA에 대해 WYZX의 순서로 정렬하는 _ACN_ channel ordering, 후자의 경우 _ambiX_ 형태로 알려진 _SN3D_ channel normalization의 방식을 이용했다.  
그러나 ambisonic은 아직 audio file container 형태로 정의된 것이 아니기 때문에 다른 surround format과 channel swapping을 헷갈리는 경우가 종종 일어난다.  
이런 channel oredering을 올바르게 하기 위해서는 channel order를 알고 있는 영상을 YouTube에 업로드 한 다음에 다운 받아서 다시 [_pyAV_](https://github.com/PyAV-Org/PyAV){:target="_blank"}를 이용하여 다시 업로드 한다.  
이 방식을 이용하면 모든 영상에 대해 올바르게 WYZX mapping이 가능하다.

그러나 여전히 많은 양의 파일이 ambisonic forat specification을 만족하지 않을 수도 있다.  
이런 일이 일어나는 이유는
- user가 ambisonic file이 아닌 stereo file로 업로드
- 잘못된 channel order로 ambisonic file을 업로드
- recording device나 encoding software가 잘못된 encoding 제공

와 같은 경우가 있다.  
YouTube에 올라오는 영상이 워낙 다양해서 이런 invalid 하는 경우가 많이 일어나는 것이다.  
따라서 이상적인 ambisonic recording과 얼마나 가까운지를 측정하는 test를 실행한다.  
$E_w=\sum_{n}w(n)^2$을 omnidirectional channel의 energy, $E_{xyz}=\sum_{n}(x(n)^2+y(n)^2+z(n)^2)$을 세 dipole channel의 energy라고 하면 thresohold인 $\tau<1$에 대해 다음 식으로 test한다.  
$$ \left | \frac{E_{xyz}}{E_w}-1 \right |\leq\tau $$  
이 식의 의미는 sound scene에서 uncorrelated sources, encoding의 orthogonality로 인한 incoherent diffuse ambience와 reverberation의 조합과 같은 여러 상황에 대해 $E_w$가 $E_{xyz}$와 최대한 같도록 하는 것이다.  
ambisonic을 녹화하는 장치는 high-frequency에서 spatial aliasing을 겪기 때문에 4kHz 기준의 low-pass filter를 통과시키고 $\tau=0.1$로 설정했더니 처음 88733개의 clip 중 28%만이 남았다.



### Audio-visual correspondence and spatial alignment
### Human action recognition
### Acoustic scene classification with ambisonic audio
### Acoustic scene classification with binaural audio

## Conclusion and Future Work
(TBA)