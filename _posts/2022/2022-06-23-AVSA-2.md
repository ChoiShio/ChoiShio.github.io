---
layout: single
title: "Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment (2)"
author: DaYun Choi
category: PaperReview
tags: audio visual self-supervised AVC AVSA classification detection
excerpt: # "First post"
date: "2022-06-23 17:40"
last_modified_at: "2022-06-24 03:40"
published: true
comments: true
toc: true
toc_sticky: true
use_kakao-sdk: true
---

## Learning from Spatial Audio Features
다음은 본 논문에서 제시하는 모델의 전체 block diagram이다.  
360도 video와 그에 상응하는 ambisonic audio가 AVC 또는 AVSA 학습 이전에 전처리 과정이 필요한데  
우선 16fps의 영상에서 0.5초 만큼, 즉 8개의 frame과 이에 상응하는 24kHz의 sampling rate를 가진 음성을 1초 만큼 준비하여 audio-video pair를 선택한다.  
360도 형태로 된 각 frame에서 YOLO를 이용하여 object detection을 수행하면 bounding box와 이에 대한 center point를 얻을 수 있다.  
이때 탐지된 여러 object 중 하나를 random으로 골라 그 방향으로 ambisonic audio signal을 beamforming 한다.  
object는 gnomonic projection 되어 360도 이미지를 평면으로 펼치고 수평으로 뒤집거나 색, 밝기, 채도 등을 바꾸어 데이터의 다양성을 증가시킨다.  
audio의 경우 log-mel이나 공간적 특징을 추출하는데 특히 log-mel spectrogram은 channel 별로 추출된다.  
이렇게 추출된 영상과 음성에 대해 AVC 또는 AVSA 학습이 진행되어 embedding을 구현할 수 있다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175316934-73d7afbf-f213-44fb-9a71-ec67d9a8ff28.png">
</p>

먼저 AVC에 대한 과정을 자세히 살펴 보자.  
AVC의 목표는 audio와 video clip의 correspondence를 기반으로 하는 feature representation을 배우는 것을 목표로 한다. (temporal overlap)  
다음 그림은 single clip에 대한 것이라 가정했을 때 하나의 crop $C$는 object detection 된 것이고, $M$은 그에 상응하는 audio feature이다.  
$C$를 입력으로 하는 video encoder는 18-layer R2+1D 구조로 되어 있다. 3D convolution을 직접 적용하는 것이 아니라 2D 다음에 1D를 적용하여 연산량을 줄인다.  
> 영상은 width, height, channel(RGB) 3개의 요소가 있으므로 총 3차원을 다루는 것이고, R은 ResNet을 의미한다.

이와 달리 $M$을 입력으로 하는 audio endocder는 9-layer 2D CNN으로 이루어져 있다.  
> 음성은 보통 오른쪽 채널과 왼쪽 채널 2개의 요소가 있다.

이렇게 encoder를 통과해서 나온 두 vector는 각각 $\mathbf{v} \in R^{512}$, $\mathbf{a} \in R^{512}$이고 다시 이를 각각 video head와 audio head를 이용하여 target embedding에 projection 시킨 것은 $\widetilde{\mathbf{v}} \in R^{128}$, $\widetilde{\mathbf{a}} \in R^{128}$이다.  
embedding의 목표를 앞서 언급했듯 서로 다른 domain에서도 적용 가능하려면 예측값이 $\widehat{\mathbf{a}} = \widetilde{\mathbf{v}}$, $\widehat{\mathbf{v}} = \widetilde{\mathbf{a}}$가 되도록 해야 한다.  
이때 $\widehat{\mathbf{a}}$와 $\widetilde{\mathbf{a}}$, $\widehat{\mathbf{v}}$와 $\widetilde{\mathbf{v}}$의 cosine similarity를 계산한 뒤 각 domain에서 cross-entropy loss를 계산하면 서로의 embedding이 얼마나 유사한지를 측정할 수 있다.  
loss를 계산할 때 $N$개의 batch 안에서 두 clip이 동일하면 1, 다르면 0으로 계산한다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175317970-2e8aca98-8438-4ca6-994f-093fced82e0f.png">
</p>

다음으로 AVSA에 대한 과정을 자세히 살펴 보자.  
AVSA의 목표는 동일한 clip 내에 여러 object와 audio signal을 사용함으로써 feature representation을 배우는 것을 목표로 한다. (temporal overlap + correct spatial orientation)  
AVSA는 크게 두 단계로 이루어져 있는데 처음에는 앞선 AVC와 동일하게 진행하여 instance level에서 correspondence를 찾고, 다음으로는 instance + crop level에서 찾는다.  
AVC와 달리 4개의 crop과 그에 상응하는 audio를 사용하는데 이는 Fig 4에서 알 수 있듯이 하나의 clip이라는 점에 유의해야 한다.
encoder를 거친 뒤 head를 이용하여 target embedding에 projection 시키는 과정은 AVC와 동일하나, transformer와 같은 translation network와 predice head를 이용하여 predicted embedding에 projection 시키는 과정이 추가되었다.  
이렇게 하는 이유는 projection 할 때 보다 정확한 translation이 가능하도록 하기 위함이라고 한다.  
cross-entropy loss를 계산할 때 $N$개의 batch 안에서 두 clip이 동일하면서 spatially aligned 되어 있으면 1, 두 clip이 같아도 misaligned 되어 있거나 다른 clip이면 0으로 계산한다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175317983-7fada214-7980-413f-8f79-958722520d23.png">
</p>

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175371969-4ecd4148-3399-4b24-a3c7-1433cca1377c.png">
</p>

## Learning from Stereo Audio
(TBA)

## Experimental Results
(TBA)

## Conclusions and Future Work
(TBA)