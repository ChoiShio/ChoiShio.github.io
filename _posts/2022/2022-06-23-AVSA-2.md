---
layout: single
title: "Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment (2)"
author: DaYun Choi
category: PaperReview
tags: audio visual self-supervised AVC AVSA classification detection
excerpt: # "First post"
date: "2022-06-23 17:40"
last_modified_at: "2022-06-24 16:40"
published: true
comments: true
toc: true
toc_sticky: true
use_kakao-sdk: true
use_math: true
---

## Learning from Spatial Audio Features
다음은 본 논문에서 제시하는 모델의 전체 block diagram이다.  
360도 video와 그에 상응하는 ambisonic audio가 AVC 또는 AVSA 학습 이전에 전처리 과정이 필요한데  
우선 16fps의 영상에서 0.5초 만큼, 즉 8개의 frame과 이에 상응하는 24kHz의 sampling rate를 가진 음성을 1초 만큼 준비하여 audio-video pair를 선택한다.  
360도 형태로 된 각 frame에서 YOLO를 이용하여 object detection을 수행하면 bounding box와 이에 대한 center point를 얻을 수 있다.  
이때 탐지된 여러 object 중 하나를 random으로 골라 그 방향으로 ambisonic audio signal을 beamforming 한다.  
object는 gnomonic projection 되어 360도 이미지를 평면으로 펼치고 수평으로 뒤집거나 색, 밝기, 채도 등을 바꾸어 데이터의 다양성을 증가시킨다.  
audio의 경우 log-mel이나 공간적 특징을 추출하는데 특히 log-mel spectrogram은 channel 별로 추출된다.  
이렇게 추출된 영상과 음성에 대해 AVC 또는 AVSA 학습이 진행되어 embedding을 구현할 수 있다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175316934-73d7afbf-f213-44fb-9a71-ec67d9a8ff28.png">
</p>

### Audio-visual correspondence learning
먼저 AVC에 대한 과정을 자세히 살펴 보자.  
AVC의 목표는 audio와 video clip의 correspondence를 기반으로 하는 feature representation을 배우는 것을 목표로 한다. (temporal overlap)  
다음 그림은 single clip에 대한 것이라 가정했을 때 하나의 crop $C$는 object detection 된 것이고, $M$은 그에 상응하는 audio feature이다.  
$C$를 입력으로 하는 video encoder는 18-layer R2+1D 구조로 되어 있다. 3D convolution을 직접 적용하는 것이 아니라 2D 다음에 1D를 적용하여 연산량을 줄인다.  
> 영상은 width, height, channel(RGB) 3개의 요소가 있으므로 총 3차원을 다루는 것이고, R은 ResNet을 의미한다.

이와 달리 $M$을 입력으로 하는 audio endocder는 9-layer 2D CNN으로 이루어져 있다.  
> 음성은 보통 오른쪽 채널과 왼쪽 채널 2개의 요소가 있다.

이렇게 encoder를 통과해서 나온 두 vector는 각각 $\mathbf{v} \in R^{512}$, $\mathbf{a} \in R^{512}$이고 다시 이를 각각 video head와 audio head를 이용하여 target embedding에 projection 시킨 것은 $\widetilde{\mathbf{v}} \in R^{128}$, $\widetilde{\mathbf{a}} \in R^{128}$이다.  
embedding의 목표를 앞서 언급했듯 서로 다른 domain에서도 적용 가능하려면 예측값이 $\widehat{\mathbf{a}} = \widetilde{\mathbf{v}}$, $\widehat{\mathbf{v}} = \widetilde{\mathbf{a}}$가 되도록 해야 한다.  
이때 $\widehat{\mathbf{a}}$와 $\widetilde{\mathbf{a}}$, $\widehat{\mathbf{v}}$와 $\widetilde{\mathbf{v}}$의 cosine similarity를 계산한 뒤 각 domain에서 cross-entropy loss를 계산하면 서로의 embedding이 얼마나 유사한지를 측정할 수 있다.  
loss를 계산할 때 $N$개의 batch 안에서 두 clip이 동일하면 1, 다르면 0으로 계산한다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175317970-2e8aca98-8438-4ca6-994f-093fced82e0f.png">
</p>

### Audio-visual spatial alignment learning
다음으로 AVSA에 대한 과정을 자세히 살펴 보자.  
AVSA의 목표는 동일한 clip 내에 여러 object와 audio signal을 사용함으로써 feature representation을 배우는 것을 목표로 한다. (temporal overlap + correct spatial orientation)  
AVSA는 크게 두 단계로 이루어져 있는데 처음에는 앞선 AVC와 동일하게 진행하여 instance level에서 correspondence를 찾고, 다음으로는 instance + crop level에서 찾는다.  
AVC와 달리 4개의 crop과 그에 상응하는 audio를 사용하는데 이는 Fig. 4에서 알 수 있듯이 하나의 clip이라는 점에 유의해야 한다.
encoder를 거친 뒤 head를 이용하여 target embedding에 projection 시키는 과정은 AVC와 동일하나, transformer와 같은 translation network와 predice head를 이용하여 predicted embedding에 projection 시키는 과정이 추가되었다.  
이렇게 하는 이유는 projection 할 때 보다 정확한 translation이 가능하도록 하기 위함이라고 한다.  
cross-entropy loss를 계산할 때 $N$개의 batch 안에서 두 clip이 동일하면서 spatially aligned 되어 있으면 1, 두 clip이 같아도 misaligned 되어 있거나 다른 clip이면 0으로 계산한다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175317983-7fada214-7980-413f-8f79-958722520d23.png">
</p>

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175371969-4ecd4148-3399-4b24-a3c7-1433cca1377c.png">
</p>

### Spatial selection of video crops with YOLO
전체적인 모델 구조는 살펴 보았으니 crop을 찾는 과정과 그에 상응하는 audio를 찾도록 하는 beamforming 과정에 대해 알아보자.  
먼저 video crop을 취득한 방법부터 살펴 보면 Fig. 5에서 보다시피 random crop을 이용하면 다양한 데이터를 얻을 수 있는 것이 장점이나 그다지 중요하지 않은 정보를 포함하고 있을 지도 모른다.  
심지어 crop과 audio가 알맞게 상응해야 하는데 object가 crop에 없으면 의미 없는 task를 하고 있을 수 있다.  
따라서 YOLO를 이용하여 object detection을 수행하면 object-oriented 특성을 띠는 crop을 선택할 수 있게 된다.  
AVC에서는 탐지된 object 중에 하나를 random으로 선택하고 만약 object가 아예 없으면 random으로 하나의 crop을 선택한다.  
AVSA에서는 탐지된 object 중에 4개의 crop을 선택하는데 각각 \[180, 90\](left-back), \[90,0\](left-front), \[0,−90\](right-front), \[−90,−180\](right-back)의 azimuth angle을 중심으로 한다. 
마찬가지로 object가 아예 없으면 random으로 하나의 crop을 선택한다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175372012-2b5cdcad-7895-4c8f-a0e6-0a6bff61dcee.png">
</p>

### Spatial sound focusing with beamforming
본 논문은 Morgado et al. 논문을 주로 참고하고 있는데 이 논문에서는 ambisonic signal을 사용하여 _mono_, _stereo_, _ambi_ 의 3가지 case를 test 했다고 한다.  
_mono_ signal의 경우 directional selectivity 없이 single-channel recording 한 것으로 ambisonic signal의 첫 번쨰 channel과 일치한다.  
그러나 Morgado et al. 논문에서 말하기를 _mono_ signal은 ambisonic signal을 사용한 beamforming의 형태이며 각각의 video crop 쪽으로 beam을 steer 할 수 있다고 한다.  
이에 대해 azimuth angle을 $\theta$, elevation angle을 $\phi$라고 하면 first-order ambisonic(FOA)은 다음과 같이 나타낼 수 있다.  
\\[ \mathbf{u}(\theta,\phi)=\begin{bmatrix} 1 & \sin\theta\cos\phi & \sin\phi & \cos\theta\cos\phi \end{bmatrix}^T \\]  
위 vector의 각 요소는 matrix 연산을 위한 값, $y$축, $z$축, $x$축 요소(Cartesian components of a unit vector pointing to the direction-of-arrival(DOA))라 생각하면 된다.  
source signal $s(n)$에 대해 ambisonic signal $\mathbf{x}(n)$은 다음과 같이 omnidirectional channel과 3개의 directional channel 성분으로 이루어졌다고 할 수 있다.  
\\[ \mathbf{x}(n)=\begin{bmatrix} w(n) & y(n) & z(n) & x(n) \end{bmatrix}^T=\mathbf{u}(\theta,\phi)s(n) \\]  
이렇게 FOA로 DOA를 직접 encode 할 수 있으며, plane-wave amplitude density를 $a(\theta,\phi,n)$이라고 하면 보다 일반적으로 continuous wave에 대해 다음과 같이 나타낼 수도 있다.  
\\[ \mathbf{x}(n)=\int_{-\pi}^{\pi}\int_{-\pi/2}^{\pi/2}\mathbf{u}(\theta,\phi)a(\theta,\phi,n)\cos\phi\mathrm{d}\phi\mathrm{d}\theta \\]  
ambisonic signal을 이용한 beamforming은 보통 weighted version이며 그 방향이 $(\theta_0,\phi_0)$라고 하면 그때의 beamformed signal은 다음과 같이 나타낼 수 있다. 이게 바로 Morgado et al. 논문에서 말한 _mono_ signal의 beamforming이다.  
\\[ y(n)=\mathbf{u}^T(\theta_0,\phi_0)\mathbf{x}(n) \\]  
ambisonic signal이 sound field 상에서 회전할 수도 있는데 FOA인 경우 ($y$축, $z$축, $x$축) 상에서 ($\alpha$, $\beta$, $\gamma$) 만큼 회전시키는 matrix를 $\mathbf{Q}$라고 하면 이때의 ambisonic signal은 다음과 같다.  
\\[ \mathbf{x}_{rot}(n)=\mathbf{Q}(\alpha, \beta, \gamma)\mathbf{x}(n) \\]  

지금까지는 _mono_ case에 대해 살펴 봤다면 _ambi_ case의 경우 $\mathbf{Q}(\theta_0,-\phi_0,0)$를 이용하여 ambisonic recording을 완전히 회전시켜 얻을 수 있다.  
이에 대한 그림은 다음 그림 Fig. 6를 보면 된다.

<p align="center">
    <img src = "https://user-images.githubusercontent.com/74304696/175372038-b5b93ed3-132a-464c-a035-4a57e08abc83.png">
</p>

direct beamforming을 입력으로 하면 _stereo_ 나 _ambi_ 에 비해 장점이 없어서 Morgado et al. 논문은 crop 당 single channel이 소리 간의 spatial relation을 나타내기에는 충분하지 않다고 결론 지었다.  
이때 crop은 video에서 random으로 선택되기 때문에 beam이 source sound가 없는 곳을 향할 수도 있어서 audio signal이 visible이든 non-visible이든 이에 의해 생성된 ambience로 주로 이루어질지도 모른다.  
한편 beamformed signal이 beamformed direction으로부터 오는 강한 source contribution을 capture하고 있을지라도 random으로 선택된 crop이 non-informative 한 background의 patch를 고립시키면서 spatially aligned가 잘 안 될 수도 있다.  
따라서 이러한 현상을 줄이기 위해 본 논문은 YOLO를 이용하여 object-oriented crop과 결합한 beamforming을 사용하고자 한 것이다.

### Spatial audio features
audio의 spatial information은 $t$를 time index, $f$를 mel-band index라고 했을 때 다음과 같다.  
\\[ \mathbf{x_{tf}^{mel}}=\begin{bmatrix} w_{tf}^{mel} & y_{tf}^{mel} & z_{tf}^{mel} & x_{tf}^{mel} \end{bmatrix}^T \\]  
각 요소는 mel-band energy이며, 이는 부분적으로 ambisonic signal 사이나 stereo에서 왼쪽과 오른쪽 channel 사이의 inter-channel level difference를 나타내기에 적합하다.  
그러나 ambisonic의 경우 mel-band energy를 이용하여 signal의 polarity가 사라지기 때문에 directional information을 잃어버릴 수도 있다.

따라서 ambisonic signal을 보다 적절하게 나타낼 수 있는 것은 active intensity vector (AIV)이며, sound energy의 평균 흐름을 나타내는 acoustic quantity이다.  
본 논문은 다음과 같은 normalize 된 AIV를 사용한다. ($\mathbb{R}$ 은 real number임을 의미)  
\\[ \mathbf{i_{tf}}=\frac{2\mathbb{R}(w_{tf}^*\begin{bmatrix} x_{tf} & y_{tf} & z_{tf} \end{bmatrix}^T)}{\left | w_{tf} \right |^2+\left | x_{tf} \right |^2+\left | y_{tf} \right |^2+\left | z_{tf} \right |^2} \\]  
따라서 FOA-IV는 다음과 같다.  
\\[ \mathbf{x_{tf}^{sf}}=\begin{bmatrix} x_{tf}^{mel} & \mathbf{i}_{tf} \end{bmatrix}^T \\]  

## Learning from Stereo Audio
(TBA)

## Experimental Results
(TBA)

## Conclusions and Future Work
(TBA)