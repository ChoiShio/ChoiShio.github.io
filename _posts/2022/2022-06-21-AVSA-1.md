---
layout: single
title: "Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment (1)"
author: DaYun Choi
category: PaperReview
tags: audio visual self-supervised AVC AVSA classification detection
excerpt: # "First post"
date: "2022-06-21 20:30"
last_modified_at: "2022-06-27 11:00"
published: false
comments: true
toc: true
toc_sticky: true
use_kakao-sdk: true
---

논문 리뷰는 우리나라 말로 쓸 예정이다. 영어로 다 쓰면 좋겠지만 그렇게 영어 실력이 좋지는 않아서 꽤 많은 시간이 걸릴 것 같기 때문이다. 그래도 단어 자체는 영어가 많아서 비중은 꽤 많을 것으로 생각된다. 논문 구조는 그대로 따라가면서 모든 내용을 적지는 않고 내가 이해한 거에 맞게 내용을 수정할 것이다.

아무튼 이제 종강도 했으니 본격적으로 연구 주제를 정해봐야 하는데 우선 교수님께서 주신 논문을 읽어보고 내 관심 분야와 어떤 접점이 있을지 살펴 보고자 한다.

기말고사 기간 전에 간단하게 읽고 나서 지금 다시 자세히 읽어보는 중에 작성하는 것이다.  
그래서 이 포스트를 계속 업데이트 하면서 내용을 늘려나갈 예정이다. 아니면 또 새 포스트로 추가할 수도 있고..  
_혹시나 내용이 틀린 게 있으면 댓글을 달아주시거나 메일을 보내주시면 좋을 것 같아요 :)_

---

본 논문은 제목만 봐도 꽤 많은 내용을 다루고 있다는 것을 알 수 있다.  
"Self-supervised Learning", "Audio Representations", "Audio-Visual Data", "Spatial Alignment"  
딥러닝을 한 번도 배워보지 않은 사람들은 이게 다 무엇일까 생각할 수도 있다. 사실 나도 아직 audio 관련해서 딥러닝을 다루어 보지는 않았기에 생소한 주제이다. 그래도 영상과 음성을 함께 다룰 수 있는 딥러닝 주제를 연구 주제로 삼고 싶어서 이 논문이 기반이 될 것 같다.

_이제 본격적으로 본 논문의 내용을 작성하려고 하는데 아직 저도 생소한 개념이 많아서 일단 정리 차원에서 쓰는 것이니 우선 양해를 먼저 구합니다 ㅎㅎ_

## Abstract
사람이 어떤 물체가 그 위치에 있다고 인식하기 위해서는 여러 감각을 활용해야 한다.  
본 논문에서는 그 감각을 시각(visual)과 청각(audio)으로 언급하고 있다. 이 두 감각을 활용하여 그 물체에서 나오는 audio signal을 나타내기 위한 task를 수행하고자 한다.

크게 두 가지 방법이 있다.
- audio-visual correspondence (AVC)
- audio-visual spatial alignment (AVSA)

추후 더 자세히 설명하겠지만 간단히 말하면 AVC는 물체에 대한 영상과 음성의 sync를 맞추는 정도라면, AVSA는 AVC에 더해 그 물체의 위치 정보까지 파악하는 방법이라 보면 된다.

이를 수행하기 위해서는 360도 video와 ambisonic audio가 필요하다. 모든 방향의 영상과 음성 데이터가 필요하다고 생각하면 된다. 먼저 "object detection"을 이용하여 물체가 그 영상에서 어느 위치에 있는지를 보고 그 위치에 음성을 "beamforming" 하여 spatial alignment*를 시도한다.  
> spatial alignment = 영상에서 object 위치에 그 object의 음성을 공간적으로 할당  

이 두 감각을 동시에 학습시키기 위해 self-supervised learning을 진행하는 것이 본 논문의 목표이다. 다만 이때 먼저 알아야 하는 것은 음성의 공간적 특성과 형태(ambisonics, mono, stereo)이다.

예를 들어 어떤 공연을 갔는데 눈 감고 들으면 누가 어디서 피아노를 치는지, 기타를 치는지, 노래를 부르는지 알 수 없다. 분명 그 소리들은 잘 구별되는데 말이다. (물론 소음이 심하면 잘 구별이 안되기도 하고..) 그런데 눈을 뜨는 순간 그 사실은 바로 확인되고 누구인지, 어디에 있는지 한 번에 파악할 수 있다. 이렇게 multimodal 방식을 이용하여 딥러닝 모델의 성능을 더 높일 수 있기에 이와 관련된 연구가 계속해서 나오고 있다. 내가 생각하기에 "object-oriented"라는 단어가 이러한 task의 핵심 키워드가 아닐까 생각한다.

## Introduction
최근 audio classification task를 수행하기 위해 visual domain에서의 데이터 또한 사용되고 있다. 이때 audio data는 visual data에 비해 annotation cost가 많이 드는 반면 수집하는 것이 더 쉽고 빠르다. 음성이 영상에 비해 덜 직관적이기 때문에 cost가 더 많이 드는 게 아닐까 생각한다.

self-supervised learning은 특정 label 없이 딥러닝 모델을 "스스로" 학습시키는 기법이다. 그럼 특정 label이 없다는 점에서 unsupervised learning과 무슨 차이가 있느냐 생각할 수 있다. self-supervised learning은 proxy learning이라고도 불리는데, 이는 입력 데이터를 low-dimensional로 mapping하는 "embedding" 과정을 거쳐 나온 데이터의 pattern을 proxy로 삼아 학습한다. 그 proxy가 supervision이 되어 스스로 학습한다는 점이 unsupervised learning과 다르다.

audio signal의 embedding을 학습하기 위해 audio만, 또는 다른 modality와 혼합한 것을 기반으로 사용할 수 있다. 예를 들어, 어느 나라에서 말을 하든 간에 (language-agnostic) 그 말의 embedding이 audio signal로만 학습 되더라도 emotion classification이 잘 되거나, 동일한 특성을 갖는 신호의 여러 관점들의 결합을 학습시켜 데이터에 여러 augmentation 방법을 적용하더라도 그 변환에 민감하지 않도록 하는 경우가 있다. embedding 과정을 통해 서로 다른 domain이더라도 특정 label 없이 좋은 성능의 결과를 낼 수 있다는 것이 핵심이다.

많은 scientific literature는 audio-visual data를 이용하여 신호를 구분했는데 보통 AVC 기법을 사용하였다. 이는 눈에 보이는 물체의 sound activity를 시각화함으로써 공간시각적 정보를 얻을 수 있으나, 여러 개의 audio channel이 있는 경우 그 정보를 버리면서 monophonic audio로 작업한다. 이게 정확히 무슨 뜻일까 고민을 해보았는데 아무리 audio channel 개수가 많아져도 결국 신호를 구분하기 위해 하나의 channel을 이용해야 가능하다는 의미라 생각한다. 그래서 이 channel 개수를 늘려도 공간적 정보를 잘 얻기 위해서
- 움직이는 차량의 영상 정보를 transfer 시켜 stereo microphone으로부터 추출한 음성으로 그것들을 localize 할 수 있도록
- depth map이나 thermal map, 더 큰 microphone array와 같은 modality를 추가
- object detection과 360도 video로 얻은 depth map을 supervision으로 이용해 4쌍의 binaural microphone으로도 SELD(Sound Event Localization & Detection)가 가능하게

하는 등의 여러 실험이 있었다.

또 최근에는 시각 정보와 함께 공간적 음성으로부터 정보를 가져와 embedding 효과를 강하게 만드는 연구가 진행되었다. 공간적 음성(spatial audio)이라고 하니 단어가 좀 어색한데 여러 채널의 음성 정보를 습득하여 공간감을 느끼게 하는 것이라 이해했다. YouTube에서 2-channel binaural* audio로 습득한 ASMR 영상을 두 채널의 신경망에 올바른 순서 또는 뒤집은 순서로 연결시키거나, 이를 4-channel ambisonic audio로 습득한 360도 영상으로 확장시켜 contrastive learning을 진행한 연구가 있었다.  
> binaural은 사람의 머리 모양을 한 모형의 양쪽 귀에 마이크를 설치하여 녹음, 이어폰이나 헤드폰에 사용  
> stereo는 왼쪽과 오른쪽의 두 개의 채널로 2개 이상의 마이크를 설치하여 녹음, 스피커에 사용

이 방식이 유용한 embedding을 만들어내기는 하나 시각적인 측면에만 초점을 맞추어 실제로는 audio embedding을 test 할 때만 사용하기도 하고, spatial audio rendering, sound source tracking, localization 등의 방식을 이용하여야만 ambisonic audio가 사용 가능하고, 비록 ambisonic audio를 얻었어도 log-mel specturm으로 audio 입력을 표현하는 등 여러 문제가 있다.

이렇게 앞서서 몇 개의 개념과 실험들을 살펴 봤을 때 본 논문이 기여하는 바를 정리하면 다음과 같다.
- object detection과 audio beamforming을 결합하여 audio와 video modality 사이에 강력한 spatial correspondence를 주는 것
- 명시적인 공간 정보를 표현하기 위해 공간적 음성의 특징을 사용하여 audio signal을 나타내는 것
- 이를 stereo와 mono의 형태로 변환하는 것이 in-domain과 out-of-domain downstream task에 얼마나 영향을 주는지 조사

## Related work
video와 audio stream을 일치시키기 위해 AVC에 대한 많은 연구가 있어 왔다. 보통 대응되는 영상과 음성을 positive pair, 서로 다른 영상과 음성에 대해 random하게 뽑는 것을 negative pair로 삼아 학습한다. 본 논문은 이와 달리 영상에서 object를 찾아 그 방향에 대한 음성을 찾아 audio embedding이 얼마나 효과적인지를 밝히고자 한다. 즉 stream을 일치시키는 것 뿐만 아니라 공간적으로 할당을 한다는 것이다.

생각보다 내용이 길어진 것 같아서 Related work는 많이 요약했고 이후 내용은 새로운 포스트에 작성해야겠다.