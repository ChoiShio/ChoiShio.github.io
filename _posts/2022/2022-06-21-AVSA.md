---
layout: single
title: "Self-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment"
author: DaYun Choi
category: PaperReview
tags: audio visual self-supervised AVC AVSA classification detection
excerpt: # "First post"
date: "2022-06-21 20:30"
last_modified_at: "2022-06-21 20:30"
published: true
comments: true
toc: true
toc_sticky: true
use_kakao-sdk: true
---

논문 리뷰는 우리나라 말로 쓸 예정이다.  
영어로 다 쓰면 좋겠지만 그렇게 영어 실력이 좋지는 않아서 꽤 많은 시간이 걸릴 것 같기 때문이다.  
그래도 단어 자체는 영어가 많아서 비중은 꽤 많을 것으로 생각된다.
논문 구조는 그대로 따라가면서 모든 내용을 적지는 않고 내가 이해한 거에 맞게 내용을 수정할 것이다.

아무튼 이제 종강도 했으니 본격적으로 연구 주제를 정해봐야 하는데  
우선 교수님께서 주신 논문을 읽어보고 내 관심 분야와 어떤 접점이 있을지 살펴 보고자 한다.

기말고사 기간 전에 간단하게 읽고 나서 지금 다시 자세히 읽어보는 중에 작성하는 것이다.  
그래서 이 포스트를 계속 업데이트 하면서 내용을 늘려나갈 예정이다. 아니면 또 새 포스트로 추가할 수도 있고..  
_혹시나 내용이 틀린 게 있으면 댓글을 달아주시거나 메일을 보내주시면 좋을 것 같아요 :)_

---

본 논문은 제목만 봐도 꽤 많은 내용을 다루고 있다는 것을 알 수 있다.  
"Self-supervised Learning", "Audio Representations", "Audio-Visual Data", "Spatial Alignment"  
딥러닝을 한 번도 배워보지 않은 사람들은 이게 다 무엇일까 생각할 수도 있다.  
사실 나도 아직 audio 관련해서 딥러닝을 다루어 보지는 않았기에 생소한 주제이다.  
그래도 영상과 음성을 함께 다룰 수 있는 딥러닝 주제를 연구 주제로 삼고 싶어서 이 논문이 기반이 될 것 같다.

## Abstract
사람이 어떤 물체가 그 위치에 있다고 인식하기 위해서는 여러 감각을 활용해야 한다.  
본 논문에서는 그 감각을 시각(visual)과 청각(audio)으로 언급하고 있다.  
이 두 감각을 활용하여 그 물체에서 나오는 소리를 나타내기 위한 task를 수행하고자 한다.

크게 두 가지 방법이 있다.
- audio-visual correspondence (AVC)
- audio-visual spatial alignment (AVSA)

추후 더 자세히 설명하겠지만 간단히 말하면  
AVC는 물체에 대한 영상과 음성의 sync를 맞추는 정도라면,  
AVSA는 AVC에 더해 그 물체의 위치 정보까지 파악하는 방법이라 보면 된다.

이를 수행하기 위해서는 360도 video와 ambisonic audio가 필요하다.  
모든 방향의 영상과 음성 데이터가 필요하다고 생각하면 된다.  
먼저 "object detection"을 이용하여 물체가 그 영상에서 어느 위치에 있는지를 보고  
그 위치에 음성을 "beamforming" 하여 spatial alignment*를 시도한다.  
    *spatial alignment = 영상에서 object 위치에 그 object의 음성을 공간적으로 할당  
이 두 감각을 특정 label 없이 동시에 학습시키는 self-supervised learning을 진행하는 것이 본 논문의 목표이다.  
다만 이때 먼저 알아야 하는 것은 음성의 공간적 특성과 형태(ambisonics, mono, stereo)이다.

예를 들어 어떤 공연을 갔는데 눈 감고 들으면 누가 어디서 피아노를 치는지, 기타를 치는지, 노래를 부르는지 알 수 없다.  
분명 그 소리들은 잘 구별되는데 말이다. (물론 소음이 심하면 잘 구별이 안되기도 하고..)  
그런데 눈을 뜨는 순간 그 사실은 바로 확인되고 누구인지, 어디에 있는지 한 번에 파악할 수 있다.  
이렇게 multimodal 방식을 이용하여 딥러닝 모델의 성능을 더 높일 수 있기에 이와 관련된 연구가 계속해서 나오고 있다.  
내가 생각하기에 "object-oriented"라는 단어가 이러한 task의 핵심 키워드가 아닐까 생각한다.

_이제 본격적으로 본 논문의 내용을 작성하려고 하는데 아직 저도 생소한 개념이 많아서 일단 정리 차원에서 쓰는 것이니 우선 양해를 먼저 구합니다 ㅎㅎ_

## Introduction
최근 audio classification task를 수행하기 위해 visual domain에서의 데이터 또한 사용되고 있다.  
이때 audio data는 visual data에 비해 annotation cost가 많이 드는 반면 수집하는 것이 더 쉽고 빠르다.  
음성이 영상에 비해 덜 직관적이기 때문에 cost가 더 많이 드는 게 아닐까 생각한다.

self-supervised learning은 앞에서도 언급했듯이 특정 label 없이 딥러닝 모델을 "스스로" 학습시키는 기법이다.  
proxy learning이라고도 불리는데, 이는 입력 데이터를 low-dimensional로 mapping하는 "embedding" 과정을 거쳐 나온 데이터의 pattern을 proxy로 삼아 학습하기 때문이다.

audio signal을 embedding 하기 위해 audio만 사용할 수도 있지만 다른 modality와 혼합하여 사용할 때도 있다.  
예를 들어, 어느 나라에서 말을 하든 간에 (language-agnostic) 그 말의 의미를 파악하기 위해 emotion classification을 하거나,  
동일한 특성을 갖는 신호의 여러 관점들의 결합을 학습시켜 데이터에 여러 augmentation 방법을 적용하더라도 그 변환에 민감하지 않도록 하는 경우가 있다.

많은 scientific literature도 audio-visual data를 이용하여 신호를 구분했는데 보통 AVC 기법을 사용하였다.  
이는 눈에 보이는 물체의 sound activity를 시각화함으로써 공간시각적 정보를 얻을 수 있으나 여러 개의 audio channel이 있는 경우 그 정보를 버리면서 monophonic audio로 작업한다.  
사실 위 줄은 거의 해석한 것이고 정확히 무슨 뜻일까 고민을 해보았는데 audio channel이 많으면 


## Related work
(TBA)

## Learning from Spatial Audio Features
(TBA)

## Learning from Stereo Audio
(TBA)

## Experimental Results
(TBA)

## Conclusions and Future Work
(TBA)